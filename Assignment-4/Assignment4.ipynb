{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dc36924",
   "metadata": {},
   "source": [
    "Part 1 : Write a Python program to scrape all available books from the website "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1160d56f",
   "metadata": {},
   "source": [
    "(a) Title "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d746936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped page 1\n",
      "Scraped page 2\n",
      "Scraped page 3\n",
      "Scraped page 4\n",
      "Scraped page 5\n",
      "Scraped page 6\n",
      "Scraped page 7\n",
      "Scraped page 8\n",
      "Scraped page 9\n",
      "Scraped page 10\n",
      "Scraped page 11\n",
      "Scraped page 12\n",
      "Scraped page 13\n",
      "Scraped page 14\n",
      "Scraped page 15\n",
      "Scraped page 16\n",
      "Scraped page 17\n",
      "Scraped page 18\n",
      "Scraped page 19\n",
      "Scraped page 20\n",
      "Scraped page 21\n",
      "Scraped page 22\n",
      "Scraped page 23\n",
      "Scraped page 24\n",
      "Scraped page 25\n",
      "Scraped page 26\n",
      "Scraped page 27\n",
      "Scraped page 28\n",
      "Scraped page 29\n",
      "Scraped page 30\n",
      "Scraped page 31\n",
      "Scraped page 32\n",
      "Scraped page 33\n",
      "Scraped page 34\n",
      "Scraped page 35\n",
      "Scraped page 36\n",
      "Scraped page 37\n",
      "Scraped page 38\n",
      "Scraped page 39\n",
      "Scraped page 40\n",
      "Scraped page 41\n",
      "Scraped page 42\n",
      "Scraped page 43\n",
      "Scraped page 44\n",
      "Scraped page 45\n",
      "Scraped page 46\n",
      "Scraped page 47\n",
      "Scraped page 48\n",
      "Scraped page 49\n",
      "Scraped page 50\n",
      "✅ Scraping finished! Total books scraped: 1000\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Website URL pattern (pages go from 1 to 50)\n",
    "base_url = \"https://books.toscrape.com/catalogue/page-{}.html\"\n",
    "\n",
    "all_books = []  # to store all book details\n",
    "\n",
    "# Loop through pages (1 to 50)\n",
    "for page in range(1, 51):\n",
    "    url = base_url.format(page)\n",
    "    response = requests.get(url)   # download the page\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # find all book containers\n",
    "    books = soup.find_all(\"article\", class_=\"product_pod\")\n",
    "\n",
    "    # if no books found, stop the loop\n",
    "    if not books:\n",
    "        break\n",
    "\n",
    "    # extract details of each book\n",
    "    for book in books:\n",
    "        title = book.h3.a[\"title\"]  # book title\n",
    "        price = book.find(\"p\", class_=\"price_color\").text.strip()\n",
    "        availability = book.find(\"p\", class_=\"instock availability\").text.strip()\n",
    "        star_class = book.find(\"p\", class_=\"star-rating\")[\"class\"]\n",
    "        star_rating = [c for c in star_class if c != \"star-rating\"][0]\n",
    "\n",
    "        all_books.append({\n",
    "            \"Title\": title,\n",
    "            \"Price\": price,\n",
    "            \"Availability\": availability,\n",
    "            \"Star Rating\": star_rating\n",
    "        })\n",
    "\n",
    "    print(f\"Scraped page {page}\")  # just to see progress\n",
    "\n",
    "# Convert list to DataFrame\n",
    "df = pd.DataFrame(all_books)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"books.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"✅ Scraping finished! Total books scraped:\", len(df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e185a54",
   "metadata": {},
   "source": [
    "Q2. Write a Python program to scrape the IMDB Top 250 Movies list \n",
    "(https://www.imdb.com/chart/top/) . For each movie, extract the following details: \n",
    "1. Rank (1–250) \n",
    "2. Movie Title \n",
    "3. Year of Release \n",
    "4. IMDB Rating \n",
    "\n",
    "Store the results in a Pandas DataFrame and export it to a CSV file named imdb_top250.csv. \n",
    "(Note: Use Selenium/Playwright to scrape the required details from this website) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7b3a63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scraping finished! imdb_top250.csv created successfully.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Start the Selenium WebDriver (here we use Chrome)\n",
    "# Make sure you have Chrome + ChromeDriver installed\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open IMDb Top 250 page\n",
    "url = \"https://www.imdb.com/chart/top/\"\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for page to load\n",
    "time.sleep(3)\n",
    "\n",
    "# Find all movie rows\n",
    "movies = driver.find_elements(By.CSS_SELECTOR, \"li.ipc-metadata-list-summary-item\")\n",
    "\n",
    "data = []\n",
    "\n",
    "for rank, movie in enumerate(movies, start=1):\n",
    "    # Movie Title\n",
    "    title = movie.find_element(By.CSS_SELECTOR, \"h3\").text\n",
    "    \n",
    "    # Extract year (inside span)\n",
    "    year = movie.find_element(By.CSS_SELECTOR, \"span.cli-title-metadata-item\").text\n",
    "    \n",
    "    # IMDB Rating\n",
    "    rating = movie.find_element(By.CSS_SELECTOR, \"span.ipc-rating-star--imdb\").text\n",
    "    \n",
    "    data.append({\n",
    "        \"Rank\": rank,\n",
    "        \"Title\": title,\n",
    "        \"Year\": year,\n",
    "        \"IMDB Rating\": rating\n",
    "    })\n",
    "\n",
    "# Close browser\n",
    "driver.quit()\n",
    "\n",
    "# Save to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"imdb_top250.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"✅ Scraping finished! imdb_top250.csv created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0943a3c",
   "metadata": {},
   "source": [
    "Q3. Write a Python program to scrape the weather information for top world cities from the \n",
    "given website (https://www.timeanddate.com/weather/) . For each city, extract the following \n",
    "details: \n",
    "1. City Name \n",
    "2. Temperature \n",
    "3. Weather Condition (e.g., Clear, Cloudy, Rainy, etc.) \n",
    "\n",
    "Store the results in a Pandas DataFrame and export it to a CSV file named weather.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31909977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scraping finished! Total cities scraped: 0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Correct URL with world cities weather\n",
    "url = \"https://www.timeanddate.com/weather/?sort=1\"\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find the weather table\n",
    "table = soup.find(\"table\", class_=\"zebra tb-wt fw va-m tb-hover\")\n",
    "\n",
    "data = []\n",
    "\n",
    "# Check if table exists\n",
    "if table:\n",
    "    for row in table.find_all(\"tr\")[1:]:  # skip header row\n",
    "        cols = row.find_all(\"td\")\n",
    "        if len(cols) >= 3:\n",
    "            city = cols[0].text.strip()\n",
    "            temp = cols[1].text.strip()\n",
    "            condition = cols[2].text.strip()\n",
    "\n",
    "            data.append({\n",
    "                \"City\": city,\n",
    "                \"Temperature\": temp,\n",
    "                \"Condition\": condition\n",
    "            })\n",
    "\n",
    "# Save results\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"weather.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"✅ Scraping finished! Total cities scraped:\", len(df))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
